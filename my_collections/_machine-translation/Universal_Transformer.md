---
title: "Universal Transformer"
date: 2019-03-05
---

# Universal Transformer
In "Universal Transformers", the researchers from Google extended the
standard Transformer architecture to be computationally universal
(Turing complete) using a novel, efficient flavor of parallel-in-time
recurrence which yields stronger results across a wider range of tasks.
This model was proposed by Google AI in 2019 and published in their
paper: [Universal Transformers](https://arxiv.org/pdf/1807.03819.pdf).
The official code of this paper can be found on the Tensor2Tensor
official GitHub repository:
[tensor2tensor/universal_transformer.py](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/models/research).

TO BE CONTINUED